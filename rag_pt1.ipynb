{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adb8a41",
   "metadata": {},
   "source": [
    "## Goal: build an Agent that, given a question, conductes a search query in the vector storage, then, gives an answer provided the found context.\n",
    "Note: This is a simple implementation, meaning that the retrieval process isn't structured like a conditional tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660f83c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6c57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee17b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"Molmo.pdf\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits = splitter.split_documents(docs)\n",
    "print(len(all_splits))\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25cba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, Optional, List\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n",
    "\n",
    "class Search(BaseModel):\n",
    "    query: Optional[str] = Field(description=\"Search query constructed from the user's request. If not needed, leave blank.\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    context: List[Document]\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "llm_search = llm.with_structured_output(Search)\n",
    "\n",
    "search_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"Turn the user's question into an effective search query.\"),\n",
    "    (\"user\", \"{content}\")\n",
    "])\n",
    "\n",
    "answer_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are a helpful AI assistant. Your task is to assist the user with their quesiton, using the context provided to you. If you don't know the answer, just say that you don't know. Keep your answer concise.\"),\n",
    "    (\"user\", \n",
    "    \"\"\"\n",
    "    CONTEXT:\n",
    "    {context} \n",
    "    QUESTION:\n",
    "    {question}\n",
    "    ANSWER:\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "def retrieve(state: State):\n",
    "    search_prompt = search_template.invoke({\"content\":state[\"question\"]})\n",
    "    query = llm_search.invoke(search_prompt).model_dump()[\"query\"]\n",
    "    if query:\n",
    "        print(\"Search conducted\")\n",
    "        docs = vector_store.similarity_search(query)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    else:\n",
    "        print(\"No search conducted\")\n",
    "        context = \"NO CONTEXT\"\n",
    "    return {\"query\": query, \"context\": context}\n",
    "\n",
    "def call_model(state: State):\n",
    "    answer_prompt = answer_template.invoke({\"context\": state[\"context\"], \"question\": state[\"question\"]})\n",
    "    answer = llm.invoke(answer_prompt)\n",
    "    return {\"answer\": answer.content}\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_edge(\"retrieve\", \"call_model\")\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": {\"rag1337\"}}}\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search conducted\n",
      "MolmoAct is an **open-source action reasoning model (ARM)** for robotic manipulation, designed to bridge perception and purposeful action through reasoning. It builds on **Molmo** (a vision-language model) and extends it to generate **grounded action sequences** for robots, improving adaptability, generalization, and explainability.\n",
      "\n",
      "Key features:\n",
      "- **Strong performance**: Outperforms baselines (e.g., GR00T, RT-1) on benchmarks like **LIBERO** and **SimplerEnv**, and excels in human evaluations.\n",
      "- **Explainability**: Provides visual reasoning traces for transparent decision-making.\n",
      "- **Fully open-source**: Releases model weights, training code, and datasets.\n",
      "- **Modular design**: Combines vision encoders (e.g., **SigLIP2**, **CLIP**) with LLMs (e.g., **Qwen2.5-7B**, **OLMo-7B**).\n",
      "- **Best model**: **MolmoAct-7B-D** (SigLIP2 + Qwen2.5-7B); most open variant: **MolmoAct-7B-O** (CLIP + OLMo).\n",
      "\n",
      "Goal: Serve as a blueprint for robots that reason before acting. More details: [AllenAI Blog](https://allenai.org/blog/molmoact).\n"
     ]
    }
   ],
   "source": [
    "question = \"Hey there! Do you know what MolmoAct is?\"\n",
    "output = app.invoke({\"question\": question}, config)\n",
    "print(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6487f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MolmoAct’s structured design delivers both strong performance and high explainability. On standard\n",
      "benchmarks such as LIBERO and SimplerEnv (Google Robot),MolmoAct consistently outperforms compet-\n",
      "itive baselines includingGR00T N1(NVIDIA et al., 2025),π0 and π0-FAST (Black et al.), RT-1 (Brohan\n",
      "et al., 2022), and TraceVLA (Zheng et al., 2024). In arena-style human evaluations for open-ended language\n",
      "instruction following,MolmoAct is preferred over baselines, achieving significantly higher Elo ratings. The\n",
      "model adapts to novel tasks more effectively through lightweight fine-tuning, surpassing other strong baselines\n",
      "in efficiency. Moreover, it generalizes well to diverse environments and task perturbations in both simulation\n",
      "and real-world settings. Its visual reasoning traces offer an explainable view into the model’s decision-making,\n",
      "while also enabling direct action steering by editing trajectory lines—an approach we find more reliable than\n",
      "\n",
      "while also enabling direct action steering by editing trajectory lines—an approach we find more reliable than\n",
      "language commands, which can suffer from ambiguity.\n",
      "MolmoAct is fully open in every aspect: we release the model weights, training code, and all components of\n",
      "our action reasoning dataset. We aim forMolmoAct to be more than a high-performing robotics foundation\n",
      "model that serves as a blueprint for building agents that reason, transforming perception into purposeful\n",
      "action through reasoning.\n",
      "2 MolmoAct\n",
      "MolmoAct is a fully open-source action reasoning model (ARM) for robotic manipulation. It builds on\n",
      "Molmo (Deitke et al., 2024), reusing its vision–language backbone composed of a vision encoder, a vision–\n",
      "language connector, and a large language model (LLM). While Molmo supports chain-of-thought reasoning\n",
      "for language and vision,MolmoAct extends this capability to generate grounded action sequences. The\n",
      "\n",
      "3. Vision–language Connector: pools and projects patch features into the LLM embedding space.\n",
      "4. LLM: autoregressively processes vision and text tokens.\n",
      "From this templateMolmoAct instantiates a family of models by selecting a vision encoder and an LLM while\n",
      "keeping the training recipe mainly consistent. Vision encoders include OpenAI ViT-L/14 336px CLIP and ViT-\n",
      "SO400M/14 384px SigLIP2. LLM backbones include fully open OLMo-2-1124-7B and open-weight Qwen2.5-7B.\n",
      "With the combination of ViT-SO400M/14 384px SigLIP2 with Qwen2.5-7B, we haveMolmoAct-7B-D, our\n",
      "best and demo model. With the combination of OpenAI ViT-L/14 336px CLIP with OLMo-2-1124-7B, we\n",
      "have MolmoAct-7B-O, our most open model. Note that although OpenAI ViT-L/14 336px CLIP uses\n",
      "closed data, it can be reproduced from scratch, as shown by MetaCLIP (Xu et al., 2024a).\n",
      "A.2 Image Encoding and Cropping\n",
      "Most ViTs accept square images at a fixed resolution, which is insufficient for fine-grained details. This\n",
      "\n",
      "MolmoAct\n",
      "Action Reasoning Models that can Reason in Space\n",
      "Jason Lee♥1,2∗ Jiafei Duan♥1,2∗ Haoquan Fang♥1,2∗\n",
      "Yuquan Deng♥1 Shuo Liu♥1,2 Boyang Li♥2 Bohan Fang♥2 Jieyu Zhang♥1,2 Yi Ru Wang♥1,2\n",
      "Sangho Lee 1 Winson Han 1 Wilbert Pumacay 1 Angelica Wu 2 Rose Hendrix 1 Karen Farley 1\n",
      "Eli VanderBilt1\n",
      "Ali Farhadi1,2 Dieter Fox♥1,2 Ranjay Krishna♥1,2\n",
      "1Allen Institute for AI,2University of Washington\n",
      "∗denotes equal contribution in no particular order.♥marks core contributors. See full author contributions here.\n",
      "MolmoAct: MolmoAct-7B-D-Pretrain-0812 MolmoAct-7B-D-0812 MolmoAct-7B-O-0812\n",
      "MolmoAct Data: MolmoAct-Dataset MolmoAct-Pretraining-Datasets MolmoAct-Midtraining-Datasets\n",
      "Blog: allenai.org/blog/molmoact\n",
      "Abstract\n",
      "Reasoning is central to purposeful action, yet most robotic foundation models map perception and\n",
      "instructions directly to control, which limits adaptability, generalization, and semantic grounding. We\n"
     ]
    }
   ],
   "source": [
    "print(output[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3671930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MolmoAct definition and overview\n"
     ]
    }
   ],
   "source": [
    "print(output[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa0fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7fa3bb86ad50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf7922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No search conducted\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you. How about you?\n"
     ]
    }
   ],
   "source": [
    "question = \"Hey there! How are you?\"\n",
    "output = app.invoke({\"question\": question}, config)\n",
    "print(output[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
